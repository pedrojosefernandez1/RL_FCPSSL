{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/introduccion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Problema de Aprendizaje en entornos complejos\n",
    "\n",
    "Este notebook, **introduccion.ipynb**, proporciona una visión general del problema de aprendizaje en entornos complejos donde se desarollaran distintos agentes, cada uno con un algoritmo concreto para entornos discretos y continuos.\n",
    "\n",
    "## 📌 Descripción\n",
    "\n",
    "En **aprendizaje por refuerzo (RL)**, un **agente** interactúa con un **entorno** para maximizar una recompensa acumulada. \n",
    "Primero de todo, haremos un estudio de un agente que implemente MonteCarlo y luego lo compararemos con otros métodos tabulares.\n",
    "\n",
    "## 1. Acercamiento a Gymnasium MonteCarlo\n",
    "👉 [Notebook Monte Carlo](https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/GymnasiumMonteCarlo.ipynb) 👈 \n",
    "\n",
    "Los algoritmos pueden dividirse en dos enfoques principales:\n",
    "\n",
    "## 1. Métodos Tabulares (Entornos Discretos)\n",
    "👉 [Notebook Estudio Métodos Tabulares](https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/MetodosTabulares.ipynb) 👈\n",
    "\n",
    "Estos almacenan los valores de estado-acción en una tabla y son viables cuando el espacio de estados es pequeño.\n",
    "\n",
    "- **Monte Carlo (MC)**: Actualiza valores al final de cada episodio usando la media de las recompensas obtenidas.  \n",
    "- **SARSA (State-Action-Reward-State-Action)**: Algoritmo **on-policy** que actualiza los valores de estado-acción en cada paso siguiendo la política actual.  \n",
    "- **Q-Learning**: Método **off-policy** que actualiza los valores basándose en la mejor acción posible, sin seguir la política actual.  \n",
    "   \n",
    "\n",
    "## 2. Métodos Aproximados (Espacios de Estado Grandes o Continuos)\n",
    "👉 [Notebook Estudio Métodos Aproximados](https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/MetodosAproximados.ipynb) 👈\n",
    "\n",
    "Cuando el espacio de estados es demasiado grande, se usan funciones de aproximación en lugar de tablas.\n",
    "\n",
    "- **SARSA Semi-Gradiente**: Extensión de SARSA que usa modelos lineales o redes neuronales para estimar los valores de estado-acción.  \n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

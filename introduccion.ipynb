{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/introduccion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci贸n al Problema de Aprendizaje en entornos complejos\n",
    "\n",
    "Este notebook, **introduccion.ipynb**, proporciona una visi贸n general del problema de aprendizaje en entornos complejos donde se desarollaran distintos agentes, cada uno con un algoritmo concreto para entornos discretos y continuos.\n",
    "\n",
    "##  Descripci贸n\n",
    "\n",
    "En **aprendizaje por refuerzo (RL)**, un **agente** interact煤a con un **entorno** para maximizar una recompensa acumulada. \n",
    "Primero de todo, haremos un estudio de un agente que implemente MonteCarlo y luego lo compararemos con otros m茅todos tabulares.\n",
    "\n",
    "## 1. Acercamiento a Gymnasium MonteCarlo\n",
    " [Notebook Monte Carlo](https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/GymnasiumMonteCarlo.ipynb)  \n",
    "\n",
    "Los algoritmos pueden dividirse en dos enfoques principales:\n",
    "\n",
    "## 1. M茅todos Tabulares (Entornos Discretos)\n",
    " [Notebook Estudio M茅todos Tabulares](https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/MetodosTabulares.ipynb) \n",
    "\n",
    "Estos almacenan los valores de estado-acci贸n en una tabla y son viables cuando el espacio de estados es peque帽o.\n",
    "\n",
    "- **Monte Carlo (MC)**: Actualiza valores al final de cada episodio usando la media de las recompensas obtenidas.  \n",
    "- **SARSA (State-Action-Reward-State-Action)**: Algoritmo **on-policy** que actualiza los valores de estado-acci贸n en cada paso siguiendo la pol铆tica actual.  \n",
    "- **Q-Learning**: M茅todo **off-policy** que actualiza los valores bas谩ndose en la mejor acci贸n posible, sin seguir la pol铆tica actual.  \n",
    "   \n",
    "\n",
    "## 2. M茅todos Aproximados (Espacios de Estado Grandes o Continuos)\n",
    " [Notebook Estudio M茅todos Aproximados](https://colab.research.google.com/github/pedrojosefernandez1/RL_FCPSSL/blob/main/MetodosAproximados.ipynb) \n",
    "\n",
    "Cuando el espacio de estados es demasiado grande, se usan funciones de aproximaci贸n en lugar de tablas.\n",
    "\n",
    "- **SARSA Semi-Gradiente**: Extensi贸n de SARSA que usa modelos lineales o redes neuronales para estimar los valores de estado-acci贸n.  \n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

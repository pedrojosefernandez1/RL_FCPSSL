##### agents/policies/epsilon_greedy.py #####
"""
M贸dulo: EpsilonGreedy
========================
Este m贸dulo define la clase `EpsilonGreedy`, que agrega la estrategia
蔚-greedy a cualquier agente que la necesite. Tambi茅n almacena el historial
de valores de `epsilon` para su an谩lisis.
"""

import numpy as np

class EpsilonGreedy:
    """
    Mixin para agregar comportamiento 蔚-greedy a cualquier agente.
    Maneja la selecci贸n de acciones y la reducci贸n progresiva de `epsilon`.
    """

    def __init__(self, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):
        """
        Inicializa la estrategia 蔚-greedy.
        
        Args:
            epsilon (float): Probabilidad inicial de exploraci贸n.
            epsilon_decay (float): Factor de decaimiento de epsilon.
            min_epsilon (float): Valor m铆nimo de epsilon.
        """
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.min_epsilon = min_epsilon
        self.epsilon_history = []  #  Guarda la evoluci贸n de epsilon
    def get_action(self, state, info, Q=None, Q_function=None, action_space=None):
        """
        Selecciona una acci贸n usando una pol铆tica 蔚-greedy, compatible con m茅todos tabulares y de aproximaci贸n.
        
        Args:
            state: Estado actual del entorno.
            info: Informaci贸n adicional del entorno.
            Q (np.array, opcional): Tabla de valores Q para m茅todos tabulares.
            Q_function (callable, opcional): Funci贸n de aproximaci贸n de Q (SARSA Semi-Gradiente, DQN).
            action_space (int): N煤mero total de acciones posibles.
        
        Returns:
            int: Acci贸n seleccionada.
        """
        assert (Q is None) != (Q_function is None), "Se debe proporcionar solo Q para tabulares o solo Q_function para aproximaci贸n."
    
        if Q is not None:
            return self._get_tabular_action(state, info, Q, action_space)
        
        if Q_function is not None:
            return self._get_approximation_action(state, Q_function, action_space)
        
        raise ValueError("Se debe proporcionar Q para m茅todos tabulares o Q_function para m茅todos de aproximaci贸n.")

    def _get_tabular_action(self, state, info, Q, nA):
        """
        Selecciona una acci贸n usando una pol铆tica 蔚-greedy.
        
        Args:
            state: Estado actual del entorno.
            info: Informaci贸n adicional del entorno.
            Q (np.array): Tabla de valores Q.
            nA (int): N煤mero de acciones disponibles.
        
        Returns:
            int: Acci贸n seleccionada.
        """
        if 'action_mask' in info:
            pi_A = info['action_mask'] * self.epsilon / np.sum(info['action_mask'])
            valid_actions = np.where(info['action_mask'])[0]
            best_action = valid_actions[np.argmax(Q[state, valid_actions])]
        else:
            pi_A = np.ones(nA, dtype=float) * self.epsilon / nA
            best_action = np.argmax(Q[state])
        
        pi_A[best_action] += (1.0 - self.epsilon)
        return np.random.choice(np.arange(nA), p=pi_A)
    
    def _get_approximation_action(self, state, Q_function, action_space):
        """
        Selecciona una acci贸n usando una pol铆tica 蔚-greedy en m茅todos de aproximaci贸n.
        """
        if np.random.rand() < self.epsilon:
            return np.random.choice(action_space)  # Exploraci贸n aleatoria
        
        q_values = Q_function(state)
        return np.argmax(q_values)
    
    def decay(self):
        """
        Reduce gradualmente el valor de `epsilon` y lo registra en el historial.
        """
        self.epsilon_history.append(self.epsilon)
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)

    def stats(self):
        return {'epsilon_history': self.epsilon_history}